{"cells":[{"cell_type":"markdown","source":["**IMPORTANT NOTE:** In order to make this notebook functional an *.env* file and *Introduction_to_Tableau.pdf* file are necessary which are not shared in the repository. The *.env* file contains the OpenAI API key which is private to the user and is not shared due to privacy reasons. *Introduction_to_Tableau.pdf* is the input file to make the retrieval part of the RAG which is not shared due to intellectual property rights of 365 Data Science."],"metadata":{"id":"X4cKZOR8Zh7X"},"id":"X4cKZOR8Zh7X"},{"cell_type":"code","source":["!pip install langchain==0.3.3\n","\n","!pip install langchain-chroma==0.1.4\n","!pip install langchain-community==0.3.2\n","!pip install langchain-openai==0.2.2\n","!pip install pypdf==5.0.1\n","!pip install python-dotenv==1.0.1"],"metadata":{"id":"ZrkMqvWfTJXr"},"id":"ZrkMqvWfTJXr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(f\"/content/drive\")\n","\n","\n","import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/365 Data Science LangChain course\")\n","print(\"Current directory:\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwgMgiywTMqx","executionInfo":{"status":"ok","timestamp":1742235898155,"user_tz":-60,"elapsed":22967,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}},"outputId":"f530213c-dca3-4b8a-c2d3-cd4a54109ce0"},"id":"WwgMgiywTMqx","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current directory: /content/drive/MyDrive/Colab Notebooks/365 Data Science LangChain course\n"]}]},{"cell_type":"markdown","source":["# Retrieval Augmented Generation (RAG)"],"metadata":{"id":"GPfOmGDOvb33"},"id":"GPfOmGDOvb33"},{"cell_type":"markdown","source":["Large Language Models (LLMs) have revolutionized natural language processing, enabling impressive performance on a wide range of tasks. However, they are inherently limited by the data they were trained on. Once an LLM is deployed, it lacks direct access to external, up-to-date, or domain-specific knowledge, which can lead to outdated responses, hallucinations, or an inability to answer highly specialized queries. This is where **Retrieval-Augmented Generation (RAG)** comes in.\n","\n","RAG enhances LLMs by integrating retrieval-based knowledge retrieval with generative capabilities. Instead of relying solely on the static knowledge embedded within its parameters, a RAG system first retrieves relevant documents or facts from an external knowledge base (e.g., a vector database, a document store, or an API) before generating a response. This allows the model to dynamically incorporate fresh, factual, and domain-specific information into its outputs, significantly improving accuracy, reliability, and adaptability.\n","\n","\n","**Why RAG is Necessary?**\n","\n","While traditional LLMs can generate fluent and contextually relevant responses, they suffer from key limitations:\n","\n","**Knowledge Cutoff & Staleness:** LLMs can only provide information based on the data available during their last training phase. They cannot access real-time updates, new research, or evolving knowledge.\n","\n","**Hallucination Issues:** Without external validation, LLMs sometimes generate factually incorrect or misleading responses. They can sometimes be so convincing that it may be really difficult to distinguish these hallucinations from genuine facts.\n","\n","**Scalability & Cost:** Training or fine-tuning a large model on new data is expensive, computationally intensive, and requires extensive labeled data.\n","\n","RAG mitigates these limitations by retrieving relevant documents at query time and conditioning the response on real-time data. This makes it particularly useful for applications like question-answering, chatbots, legal and medical AI assistants, and enterprise search systems.\n","\n","In the absence of RAG, we may need fine tuning the LLM to achieve similar functionality. There are some key factors that make RAG preferrable on fine tuning in many cases. Fine-tuning involves modifying an LLM's parameters by training it on new domain-specific data. However, this approach requires a full or partial (i.e. retraining only final layers) retraining process to adapt the model while preserving its general capabilities. Despite its effectiveness, fine-tuning is computationally expensive, demanding significant resources in terms of training time, hardware, and labeled data. Additionally, once a model is fine-tuned, it remains static and updating knowledge necessitates another round of training, making it costly and impractical for frequently changing domains.\n","\n","In contrast, RAG avoids these issues by retrieving external information dynamically at query time. Instead of embedding all knowledge within the model's weights, it fetches relevant documents from a database or API, allowing for real-time updates without retraining. This makes RAG a more efficient, scalable, and cost-effective solution, particularly for applications requiring up-to-date information or multi-domain adaptability.\n","\n","\n","The RAG process consists of three key steps: **Indexing**, **retrieval**, and **generation**. Each step plays a crucial role in ensuring that the model can dynamically fetch useful knowledge and incorporate it into its responses.\n","\n","* **1. Indexing:** Before an LLM can retrieve information, relevant documents or data sources need to be indexed. This step ensures that raw knowledge (such as PDFs, research papers, website content, or even images) is structured in a way that enables efficient retrieval. Indexing consists of the following sub-steps:\n","\n"," * **Loading:**  The raw data (text files, PDFs, HTML content, images, etc.) is collected and prepared for processing.\n","\n"," * **Splitting:** Since documents can be large, they are broken into smaller chunks to facilitate more efficient retrieval and matching. The chunking strategy depends on the type of content and use case.\n","\n"," * **Embedding:** This is the critical transformation step. The textual data (or other types of structured data) is converted into a numerical vector format using a pre-trained embedding model. Until this step, all data remains in its original raw form, but embedding maps it into a fixed-size (or sometimes variable-length) vector representation in a high-dimensional space. These vectorized representations capture the semantic meaning of the data, making similarity-based retrieval possible.\n","\n"," * **Storing:** The generated embeddings are stored in a vector database (e.g., FAISS, Pinecone, ChromaDB), where they can be efficiently searched based on relevance during retrieval. The original document content is also stored alongside the embeddings for reference.\n","\n","* **2. Retrieval:** When a query is received, the system retrieves the most useful pieces of information by searching the stored embeddings in the vector database. Here, \"useful\" generally means relevant (directly matching the query intent) but can also mean diverse (capturing multiple perspectives or sources, preventing duplicates). Some retrieval methods prioritize purely relevance-based ranking, while others introduce diversity-promoting techniques (e.g., Maximal Marginal Relevance (MMR)) to ensure a balance between closely matching and complementary information. This improves the robustness of responses and reduces redundancy.\n","\n","* **3. Generation:** Once useful documents are retrieved, they are passed to the LLM along with the user's query. The model then performs a forward pass over this combined input to generate a response. Crucially, this step does not involve training or weight updates—the LLM remains unchanged, merely conditioning its output on the provided context. By leveraging external information in real time, the model produces responses that are more factually grounded, context-aware, and resistant to hallucinations, without the need for expensive retraining.\n","\n","By integrating these three steps, RAG enables LLMs to stay relevant, handle domain-specific queries, and respond with real-time knowledge—without expensive retraining. This makes it a powerful alternative to traditional fine-tuning and an essential technique for AI systems requiring continuous access to evolving data."],"metadata":{"id":"9uiesnHavfqm"},"id":"9uiesnHavfqm"},{"cell_type":"markdown","id":"05930fc5-89b1-4ff5-a889-b5086431bd28","metadata":{"id":"05930fc5-89b1-4ff5-a889-b5086431bd28"},"source":["# Create a Q&A Chatbot with LangChain Project"]},{"cell_type":"markdown","id":"424c50cf-21c2-4f02-83fd-e568379eef59","metadata":{"id":"424c50cf-21c2-4f02-83fd-e568379eef59"},"source":["### Set the OpenAI API Key as an Environment Variable"]},{"cell_type":"code","execution_count":3,"id":"b8747c00-d9b7-4ac0-93d5-ef889fce6af8","metadata":{"id":"b8747c00-d9b7-4ac0-93d5-ef889fce6af8","executionInfo":{"status":"ok","timestamp":1742235898925,"user_tz":-60,"elapsed":769,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["%load_ext dotenv\n","%dotenv"]},{"cell_type":"code","source":["import os\n","import openai\n","\n","# Get the API key for OpenAI\n","openai.api_key = os.getenv(\"OPENAI_API_KEY\")"],"metadata":{"id":"dFDL8hlSTsKC","executionInfo":{"status":"ok","timestamp":1742235905017,"user_tz":-60,"elapsed":6089,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"id":"dFDL8hlSTsKC","execution_count":4,"outputs":[]},{"cell_type":"markdown","id":"ffb206df-1a4d-4cb4-aa1b-70423a43a3ae","metadata":{"id":"ffb206df-1a4d-4cb4-aa1b-70423a43a3ae"},"source":["### Import the Libraries"]},{"cell_type":"code","execution_count":5,"id":"829eb30a-8fec-4b81-bb41-056dec214c88","metadata":{"id":"829eb30a-8fec-4b81-bb41-056dec214c88","executionInfo":{"status":"ok","timestamp":1742235911115,"user_tz":-60,"elapsed":6100,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["from langchain_community.document_loaders.pdf import PyPDFLoader\n","\n","from langchain_text_splitters import (MarkdownHeaderTextSplitter,\n","                                      TokenTextSplitter)\n","\n","from langchain_core.output_parsers.string import StrOutputParser\n","from langchain_core.messages import SystemMessage\n","from langchain_core.prompts import (PromptTemplate,\n","                                    HumanMessagePromptTemplate,\n","                                    ChatPromptTemplate)\n","from langchain_core.runnables import (RunnablePassthrough,\n","                                      RunnableLambda,\n","                                      chain)\n","\n","from langchain_openai import (ChatOpenAI,\n","                              OpenAIEmbeddings)\n","\n","from langchain_chroma.vectorstores import Chroma"]},{"cell_type":"markdown","id":"c084e32c-e209-4bc1-b4d0-d8e6a2b11963","metadata":{"id":"c084e32c-e209-4bc1-b4d0-d8e6a2b11963"},"source":["### Load the Course Transcript"]},{"cell_type":"markdown","source":["This is the first step in the **indexing** process, where the document is loaded for further processing. It uses PyPDFLoader to read the PDF file *Introduction_to_Tableau.pdf*. It extracts all text into a list *docs_list* and then concatenates it into a single string *string_list_concat* for structured processing."],"metadata":{"id":"YaXdWuBd-DUz"},"id":"YaXdWuBd-DUz"},{"cell_type":"code","execution_count":6,"id":"afb7f01a-5a9b-49d2-90b6-22c681ed33f1","metadata":{"id":"afb7f01a-5a9b-49d2-90b6-22c681ed33f1","executionInfo":{"status":"ok","timestamp":1742235911276,"user_tz":-60,"elapsed":165,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["loader_pdf = PyPDFLoader(\"Introduction_to_Tableau.pdf\")"]},{"cell_type":"code","execution_count":7,"id":"8426085f-9154-405c-802f-3102aa429ce9","metadata":{"id":"8426085f-9154-405c-802f-3102aa429ce9","executionInfo":{"status":"ok","timestamp":1742235916117,"user_tz":-60,"elapsed":4839,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["docs_list = loader_pdf.load()"]},{"cell_type":"code","execution_count":8,"id":"33bd8fe6-d49c-484f-a27a-13facb3a40da","metadata":{"id":"33bd8fe6-d49c-484f-a27a-13facb3a40da","executionInfo":{"status":"ok","timestamp":1742235916120,"user_tz":-60,"elapsed":9,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["string_list_concat = \"\\n\".join([doc.page_content for doc in docs_list])"]},{"cell_type":"markdown","id":"4dbdf062-cbf6-4c93-9120-6665989056d9","metadata":{"id":"4dbdf062-cbf6-4c93-9120-6665989056d9"},"source":["### Split the Course Transcript with MarkdownHeaderTextSplitter"]},{"cell_type":"markdown","source":["This step does not split the document into smaller chunks for *indexing*. Instead, it organizes the text based on section titles and course titles. It identifies headers (# for sections, ## for course titles) to preserve document structure. The actual chunking for memory efficiency and retrieval will happen in the next steps."],"metadata":{"id":"oz4EztIt-YQ6"},"id":"oz4EztIt-YQ6"},{"cell_type":"code","execution_count":9,"id":"46e373b9-ad32-45a1-8c2c-33f7bbb2e956","metadata":{"id":"46e373b9-ad32-45a1-8c2c-33f7bbb2e956","executionInfo":{"status":"ok","timestamp":1742235916122,"user_tz":-60,"elapsed":9,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Section Title\"), (\"##\", \"Course Title\")])"]},{"cell_type":"code","execution_count":10,"id":"9b157622-d5da-4eec-af0e-f6bf3e54e1b4","metadata":{"id":"9b157622-d5da-4eec-af0e-f6bf3e54e1b4","executionInfo":{"status":"ok","timestamp":1742235916123,"user_tz":-60,"elapsed":8,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["docs_list_md_split = md_splitter.split_text(string_list_concat)"]},{"cell_type":"markdown","id":"bd2d5cf9-71dc-4293-b11e-25cbe5577766","metadata":{"id":"bd2d5cf9-71dc-4293-b11e-25cbe5577766"},"source":["### Create a Chain to Correct the Course Transcript"]},{"cell_type":"markdown","source":["This step refines the extracted transcript by fixing punctuation, formatting, and common spelling errors. Each section of the split transcript *string_list_split* is processed using an LLM (GPT-4o). A system prompt *PROMPT_FORMATTING_S* instructs the model to:\n"," * Split text into meaningful paragraphs.\n"," * Correct punctuation errors.\n"," * Fix common misinterpretations (e.g., \"tableaux\" → \"Tableau\").\n","\n","The corrected text replaces the original content in *docs_list_md_split*, ensuring cleaner, more structured text for later retrieval."],"metadata":{"id":"wyW8fJPARJmV"},"id":"wyW8fJPARJmV"},{"cell_type":"code","execution_count":11,"id":"5e47e9df-f7f9-45e1-88fb-7803af9aab1e","metadata":{"id":"5e47e9df-f7f9-45e1-88fb-7803af9aab1e","executionInfo":{"status":"ok","timestamp":1742235920457,"user_tz":-60,"elapsed":49,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["string_list_split = [doc.page_content for doc in docs_list_md_split]"]},{"cell_type":"code","execution_count":12,"id":"2e332a62-9d4a-4d00-9a07-99041b2ddd9d","metadata":{"id":"2e332a62-9d4a-4d00-9a07-99041b2ddd9d","executionInfo":{"status":"ok","timestamp":1742235922123,"user_tz":-60,"elapsed":17,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["PROMPT_FORMATTING_S = '''Improve the following Tableau lecture transcript by:\n","- Splitting the text into meaningful paragraphs\n","- Correcting any misplaced punctuation\n","- Fixing mistranscribed words (e.g., changing 'tableaux' to 'Tableau')\"\n","'''\n","\n","PROMPT_TEMPLATE_FORMATTING_H = '''This is the transcript:\n","{lecture_transcript}\n","'''"]},{"cell_type":"code","execution_count":18,"id":"fecd79f3-4cca-4d11-9bb7-b7714b7dbb59","metadata":{"id":"fecd79f3-4cca-4d11-9bb7-b7714b7dbb59","executionInfo":{"status":"ok","timestamp":1742236921527,"user_tz":-60,"elapsed":10,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["prompt_formatting_s = PROMPT_FORMATTING_S\n","prompt_template_formatting_h = PROMPT_TEMPLATE_FORMATTING_H\n","chat_prompt_template_formatting = ChatPromptTemplate.from_messages([(\"system\", prompt_formatting_s), (\"human\", prompt_template_formatting_h)])"]},{"cell_type":"code","execution_count":19,"id":"538c92fb-adea-425c-b40e-6aa2839077f3","metadata":{"id":"538c92fb-adea-425c-b40e-6aa2839077f3","executionInfo":{"status":"ok","timestamp":1742236925244,"user_tz":-60,"elapsed":444,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["chat = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, seed=365)"]},{"cell_type":"code","execution_count":20,"id":"5af89457-8698-4e60-8de0-24cd55a0555a","metadata":{"id":"5af89457-8698-4e60-8de0-24cd55a0555a","executionInfo":{"status":"ok","timestamp":1742236926331,"user_tz":-60,"elapsed":5,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["str_output_parser = StrOutputParser()"]},{"cell_type":"code","execution_count":21,"id":"79906ac0-08e2-48d5-bf34-ce80960caee1","metadata":{"id":"79906ac0-08e2-48d5-bf34-ce80960caee1","executionInfo":{"status":"ok","timestamp":1742236927118,"user_tz":-60,"elapsed":18,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["chain_formatting = (chat_prompt_template_formatting | chat | str_output_parser)"]},{"cell_type":"code","execution_count":22,"id":"72fc1a0e-92e9-43f5-ac5a-8d813e783937","metadata":{"id":"72fc1a0e-92e9-43f5-ac5a-8d813e783937","executionInfo":{"status":"ok","timestamp":1742237001161,"user_tz":-60,"elapsed":73452,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["string_list_formatted = chain_formatting.batch([{\"lecture_transcript\": text} for text in string_list_split])"]},{"cell_type":"code","execution_count":23,"id":"ef0c01db-4683-4b81-baa8-e1676bfc3c73","metadata":{"id":"ef0c01db-4683-4b81-baa8-e1676bfc3c73","executionInfo":{"status":"ok","timestamp":1742237001169,"user_tz":-60,"elapsed":3,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["# Override the docs_list_md_split list such that the page_content parameter of each Document objects stores the updated lecture.\n","for i, doc in enumerate(docs_list_md_split):\n","    doc.page_content = string_list_formatted[i]"]},{"cell_type":"code","execution_count":null,"id":"1b4e8ee8-8e0b-46f7-9b9c-7f90699aa6aa","metadata":{"id":"1b4e8ee8-8e0b-46f7-9b9c-7f90699aa6aa"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2c8dc5aa-cb78-414a-8b41-c60a5d74bc3e","metadata":{"id":"2c8dc5aa-cb78-414a-8b41-c60a5d74bc3e"},"source":["### Split the Lectures with TokenTextSplitter"]},{"cell_type":"markdown","source":["This step splits the cleaned transcript into smaller chunks for efficient retrieval. Uses *TokenTextSplitter* with a chunk size of 500 tokens and 50-token overlap to maintain context between chunks. This is a crucial step in indexing, ensuring each chunk is manageable for embedding and retrieval."],"metadata":{"id":"q3vq_5aUSKCL"},"id":"q3vq_5aUSKCL"},{"cell_type":"code","execution_count":24,"id":"edbe0b3d-f78a-4b78-89c3-7a6ac09ffef6","metadata":{"id":"edbe0b3d-f78a-4b78-89c3-7a6ac09ffef6","executionInfo":{"status":"ok","timestamp":1742237024563,"user_tz":-60,"elapsed":1900,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["token_splitter = TokenTextSplitter(encoding_name=\"cl100k_base\", chunk_size=500, chunk_overlap=50)"]},{"cell_type":"code","execution_count":25,"id":"95238c0d-3d62-425f-93a9-10443913ad48","metadata":{"id":"95238c0d-3d62-425f-93a9-10443913ad48","executionInfo":{"status":"ok","timestamp":1742237024601,"user_tz":-60,"elapsed":12,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["docs_list_tokens_split = token_splitter.split_documents(docs_list_md_split)"]},{"cell_type":"markdown","id":"d0886d56-6d15-4e15-850f-e7e12b8b26a1","metadata":{"id":"d0886d56-6d15-4e15-850f-e7e12b8b26a1"},"source":["### Create Embeddings, Vector Store, and Retriever"]},{"cell_type":"markdown","source":["Here the text chunks are converted into vector embeddings using OpenAI's \"text-embedding-3-small\" model. The vector embeddings are stored in a ChromaDB vector store, persisting them for future queries. With these two steps, indexing part is finished.\n","\n","Then, we go to the 2nd step of the RAG which is retrival. A retriever is set up with MMR (Maximal Marginal Relevance) search, which balances relevance and diversity in retrieved results.\n","\n","Let's also delve into how MMR works briefly here. Given a query $q$, a set of candidate documents $D$, and already selected documents $S$, MMR selects the next document $d^*$ by:\n","\n","$$\n","d^* = \\arg\\max_{d \\in D \\setminus S} \\left[ \\lambda \\cdot \\text{sim}(d, q) - (1 - \\lambda) \\cdot \\max_{s \\in S} \\text{sim}(d, s) \\right]\n","$$\n","\n","where:\n","- $ \\lambda $ (**0.7 in this example**) controls the trade-off:\n","  - **Higher $ \\lambda $** → More relevance to the query.\n","  - **Lower $ \\lambda $** → More diversity in results.\n","- $ \\text{sim}(x, y) $ is the similarity function (e.g., cosine similarity)."],"metadata":{"id":"Gg6CpfKQSS_s"},"id":"Gg6CpfKQSS_s"},{"cell_type":"code","execution_count":26,"id":"249f1585-2281-4f20-a0d9-cf90934d906e","metadata":{"id":"249f1585-2281-4f20-a0d9-cf90934d906e","executionInfo":{"status":"ok","timestamp":1742237029374,"user_tz":-60,"elapsed":248,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")"]},{"cell_type":"code","execution_count":27,"id":"dcdeb49d-94a2-403f-897d-544c05474195","metadata":{"id":"dcdeb49d-94a2-403f-897d-544c05474195","executionInfo":{"status":"ok","timestamp":1742237034129,"user_tz":-60,"elapsed":4286,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["vectorstore = Chroma.from_documents(documents=docs_list_tokens_split, embedding=embedding, persist_directory=\"./intro-to-tableau\")"]},{"cell_type":"code","execution_count":28,"id":"bd3cfdf2-9d32-423f-af0b-fce347136d8a","metadata":{"id":"bd3cfdf2-9d32-423f-af0b-fce347136d8a","executionInfo":{"status":"ok","timestamp":1742237034135,"user_tz":-60,"elapsed":3,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"lambda_mult\": 0.7})"]},{"cell_type":"markdown","id":"1d10e4ac-b6ed-4d7b-9512-33db50f4f49d","metadata":{"id":"1d10e4ac-b6ed-4d7b-9512-33db50f4f49d"},"source":["### Create Prompts and Prompt Templates for the Q&A Chatbot Chain"]},{"cell_type":"markdown","source":["This step defines structured prompt templates to guide the LLM when answering questions using retrieved content. Three prompts are created:\n"," * PROMPT_CREATING_QUESTION → Formats a question with a lecture reference, ensuring clarity in the Q&A process.\n"," * PROMPT_RETRIEVING_S → Instructs the LLM to answer using only the provided context from the lecture. This ensures responses are grounded in retrieved content and include section and lecture citations.\n"," * PROMPT_TEMPLATE_RETRIEVING_H → Defines the structure for passing the retrieved context and question to the model."],"metadata":{"id":"8ybmO6zGWMKf"},"id":"8ybmO6zGWMKf"},{"cell_type":"code","execution_count":31,"id":"f1833588-4edc-4a10-bde4-81e889708834","metadata":{"id":"f1833588-4edc-4a10-bde4-81e889708834","executionInfo":{"status":"ok","timestamp":1742237515516,"user_tz":-60,"elapsed":8,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["PROMPT_CREATING_QUESTION = '''Lecture: {question_lecture}\n","Title: {question_title}\n","Body: {question_body}'''\n","\n","PROMPT_RETRIEVING_S = '''You will receive a question from a student taking a Tableau course, which includes a title and a body.\n","The corresponding lecture will also be provided.\n","\n","Answer the question using only the provided context.\n","\n","At the end of your response, include the section and lecture names where the context was drawn from, formatted as follows:\n","Resources:\n","Section: *Section Title*, Lecture: *Lecture Title*\n","...\n","Replace *Section Title* and *Lecture Title* with the appropriate titles.'''\n","\n","PROMPT_TEMPLATE_RETRIEVING_H = '''This is the question:\n","{question}\n","\n","This is the context:\n","{context}'''\n","\n","prompt_creating_question = PromptTemplate.from_template(PROMPT_CREATING_QUESTION)\n","prompt_retrieving_s = PROMPT_RETRIEVING_S\n","prompt_template_retrieving_h = PROMPT_TEMPLATE_RETRIEVING_H\n","\n","chat_prompt_template_retrieving = ChatPromptTemplate.from_messages([(\"system\", prompt_retrieving_s), (\"human\", prompt_template_retrieving_h)])"]},{"cell_type":"markdown","id":"cdf46c96-bd27-4868-a943-4338fa3e72fb","metadata":{"id":"cdf46c96-bd27-4868-a943-4338fa3e72fb"},"source":["### Create the First Version of the Q&A Chatbot Chain"]},{"cell_type":"markdown","source":["The retrieval chain is built by linking the following steps:\n"," * Formatting the input question with prompt_creating_question.\n"," * Using a retriever to fetch the most relevant lecture sections.\n"," * Structuring the final input using *chat_prompt_template_retrieving*."],"metadata":{"id":"mz0ppkobXqR7"},"id":"mz0ppkobXqR7"},{"cell_type":"code","execution_count":34,"id":"e7151feb-4107-4f7a-bc97-be31262ed7ec","metadata":{"id":"e7151feb-4107-4f7a-bc97-be31262ed7ec","executionInfo":{"status":"ok","timestamp":1742237620861,"user_tz":-60,"elapsed":4,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["chain_retrieving = (\n","    prompt_creating_question\n","    | (lambda x: x.text)\n","    | (lambda q: {\"question\": q, \"context\": retriever.invoke(q)})\n","    | chat_prompt_template_retrieving\n","    )"]},{"cell_type":"code","execution_count":35,"id":"02434124-5f37-4b3e-a8d4-21c702e2124e","metadata":{"id":"02434124-5f37-4b3e-a8d4-21c702e2124e","executionInfo":{"status":"ok","timestamp":1742237622853,"user_tz":-60,"elapsed":736,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["result = chain_retrieving.invoke({\"question_lecture\": \"Adding a custom calculation\",\n","                                  \"question_title\": \"Why are we using SUM here? It's unclear to me.\",\n","                                  \"question_body\": \"This question refers to calculating the GM%.\"})"]},{"cell_type":"code","execution_count":36,"id":"b1356e39-14cc-4cbc-b3a4-cc05a3f919f8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1356e39-14cc-4cbc-b3a4-cc05a3f919f8","executionInfo":{"status":"ok","timestamp":1742237625301,"user_tz":-60,"elapsed":10,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}},"outputId":"5b1d367f-9309-48a2-bff9-68f99afae232"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You will receive a question from a student taking a Tableau course, which includes a title and a body. \\nThe corresponding lecture will also be provided.\\n\\nAnswer the question using only the provided context.\\n\\nAt the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \\nResources: \\nSection: *Section Title*, Lecture: *Lecture Title* \\n...\\nReplace *Section Title* and *Lecture Title* with the appropriate titles.', additional_kwargs={}, response_metadata={}), HumanMessage(content='This is the question:\\nLecture: Adding a custom calculation\\nTitle: Why are we using SUM here? It\\'s unclear to me.\\nBody: This question refers to calculating the GM%.\\n\\nThis is the context:\\n[Document(metadata={\\'Course Title\\': \\'Adding a custom calculation\\', \\'Section Title\\': \\'Tableau Functionalities\\'}, page_content=\"Ok, excellent. We\\'re doing good. We\\'ve seen quite a few interesting Tableau tools so far, and we\\'ll continue to do so during this lesson as well.\\\\n\\\\nOur table is almost ready. We have revenue, COGS, and gross profit. Now that I think about it, one thing we should probably add is a gross margin calculation right next to the gross profit figures. Gross margin is useful because it allows us to see what portion of revenues were converted into gross profit once we have considered the cost of goods sold.\\\\n\\\\nRight. Let\\'s add a new calculated field. I\\'ll name it GM percent. All we have to do is divide gross profits by revenue, right? And we already know how to do that. Ok, here we are. We\\'ve calculated a new field. Let\\'s add it to the table. I\\'ll insert it in the measure values card there. I\\'ve added the new field right next to gross profit.\\\\n\\\\nBut it looks strange, doesn\\'t it? If we divide gross profit by revenues, we would usually expect a number in the region of 10, 20, 30, or maybe 50%. Certainly not 1000. What is going on here? When I divide 71 million by 244 million, which is what we have in January, I obtain 29%, approximately.\\\\n\\\\nThere are two possible explanations. Either Tableau miscalculated the simple division we asked it to perform, or our formula is not 100% correct. It\\'s most likely us and not the computer, right? I\\'ll take out the GM percent field from the measure values box, and we\\'ll edit the calculated field from here.\\\\n\\\\nWhat we forgot to do is type SUM around the two variables. If we don\\'t sum the variables, we are not dividing their total figures for each month. Let\\'s adjust our calculated field in this way and see what happens. OK?\\\\n\\\\nThis is a column with numbers that look like zeros, but perhaps these are percentage values. Let\\'s change the way the GM percent column is displayed. To do that, I\\'ll simply click on the GM percent variable in the measure values card and select the format option. We have quite a few options available here, so I\\'ll simply select a percentage format with one decimal place. Voila, our table is ready.\\\\n\\\\nIn our next lesson, we\\'ll add a filter that would allow us to choose whether to see both 2016 and 2017 values or just one\"), Document(metadata={\\'Course Title\\': \\'Adding totals and subtotals\\', \\'Section Title\\': \\'Tableau Functionalities\\'}, page_content=\"Most executives, and by most, I mean all of them, prefer receiving tables that contain totals and subtotals. This makes it easier for them and helps them digest information in a faster way. In this lesson, we will learn how to add totals and subtotals to our Tableau tables.\\\\n\\\\nThat\\'s nice because the table we have been working on in the last few videos doesn\\'t contain totals for 2016 and 2017. We\\'ll add them in this video. That\\'s fairly easy to do. I\\'ll go to Analysis, then Totals, and we\\'ll select Show Column Grand Totals.\\\\n\\\\nThe totals we just added are for the entire period of 2016 and 2017. That\\'s not really useful, is it? Whenever a person is interested in a company\\'s sales or gross profit, they want to know how much it made in a specific year and whether it performed better than the year before. A company will also want to know how its year\\'s sales compare to other companies.\\\\n\\\\nSo, I\\'ll go to Analysis, Totals, and unclick the Show Column Grand Totals to remove the total we just added. What I can do differently the second time around is opt for Add All Subtotals. This will give me the total figures for 2016 and 2017 separately. Quite nice, right?\\\\n\\\\nNow, we can read the table easily. The fictitious numbers we see here help us compare 2016 and 2017 and give us a good idea of what the actual sales were for that period. In our next lesson, we\\'ll add percentage gross margin to our table. Thanks for watching.\")]', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":36}],"source":["result"]},{"cell_type":"markdown","id":"6150053b-f05c-4d6f-b309-b9f471144c13","metadata":{"id":"6150053b-f05c-4d6f-b309-b9f471144c13"},"source":["### Create a Runnable Function to Format the Context"]},{"cell_type":"markdown","source":["A new function *format_context* is introduced to format retrieved content for better readability and structured output. Instead of raw text chunks, it organizes context with:\n"," * Document index\n"," * Section title\n"," * Lecture title\n"," * Content text\n","\n","### How is the Chain Improved?\n","\n","The retrieval process remains the same, but now the retrieved content is structured before being passed to the prompt. The updated chain *chain_retrieving_improved* includes:\n"," * Formatting the user question\n"," * Retrieving relevant lecture sections\n"," * Applying format_context to organize retrieved text\n"," * Feeding structured data into the prompt template"],"metadata":{"id":"Almzf8cjYHLf"},"id":"Almzf8cjYHLf"},{"cell_type":"code","execution_count":37,"id":"f69a8cb6-f8d9-49e9-a0ef-6acb7c995534","metadata":{"id":"f69a8cb6-f8d9-49e9-a0ef-6acb7c995534","executionInfo":{"status":"ok","timestamp":1742237866692,"user_tz":-60,"elapsed":5,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["def format_context(dictionary):\n","    formatted_context = \"\\n----------------------\\n\".join(\n","        f\"Document {i+1}\\nSection Title: {doc.metadata.get('section_title', 'N/A')}\\n\"\n","        f\"Lecture Title: {doc.metadata.get('lecture_title', 'N/A')}\\n\"\n","        f\"Content: {doc.page_content}\"\n","        for i, doc in enumerate(dictionary[\"context\"])\n","    )\n","\n","    return {\"context\": formatted_context, \"question\": dictionary[\"question\"]}"]},{"cell_type":"code","execution_count":38,"id":"03fbad37-bae4-49be-9d59-842b07618427","metadata":{"id":"03fbad37-bae4-49be-9d59-842b07618427","executionInfo":{"status":"ok","timestamp":1742237867811,"user_tz":-60,"elapsed":11,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["chain_retrieving_improved = (\n","    prompt_creating_question\n","    | RunnableLambda(lambda x: x.text)\n","    | RunnableLambda(lambda q: {\"question\": q, \"context\": retriever.invoke(q)})\n","    | RunnableLambda(format_context)\n","    | chat_prompt_template_retrieving\n",")"]},{"cell_type":"code","execution_count":39,"id":"4495528d-bc83-44c4-a0c6-a673563cb5b1","metadata":{"id":"4495528d-bc83-44c4-a0c6-a673563cb5b1","executionInfo":{"status":"ok","timestamp":1742237870944,"user_tz":-60,"elapsed":593,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["result_improved = chain_retrieving_improved.invoke({\"question_lecture\": \"Adding a custom calculation\",\n","                                                    \"question_title\": \"Why are we using SUM here? It's unclear to me.\",\n","                                                    \"question_body\": \"This question refers to calculating the GM%.\"})"]},{"cell_type":"code","execution_count":40,"id":"8f2fcd07-2fdd-4d82-8210-d6580f21b659","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f2fcd07-2fdd-4d82-8210-d6580f21b659","executionInfo":{"status":"ok","timestamp":1742237873816,"user_tz":-60,"elapsed":34,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}},"outputId":"62eff686-0697-409f-b64d-384e00098c50"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You will receive a question from a student taking a Tableau course, which includes a title and a body. \\nThe corresponding lecture will also be provided.\\n\\nAnswer the question using only the provided context.\\n\\nAt the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \\nResources: \\nSection: *Section Title*, Lecture: *Lecture Title* \\n...\\nReplace *Section Title* and *Lecture Title* with the appropriate titles.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"This is the question:\\nLecture: Adding a custom calculation\\nTitle: Why are we using SUM here? It's unclear to me.\\nBody: This question refers to calculating the GM%.\\n\\nThis is the context:\\nDocument 1\\nSection Title: N/A\\nLecture Title: N/A\\nContent: Ok, excellent. We're doing good. We've seen quite a few interesting Tableau tools so far, and we'll continue to do so during this lesson as well.\\n\\nOur table is almost ready. We have revenue, COGS, and gross profit. Now that I think about it, one thing we should probably add is a gross margin calculation right next to the gross profit figures. Gross margin is useful because it allows us to see what portion of revenues were converted into gross profit once we have considered the cost of goods sold.\\n\\nRight. Let's add a new calculated field. I'll name it GM percent. All we have to do is divide gross profits by revenue, right? And we already know how to do that. Ok, here we are. We've calculated a new field. Let's add it to the table. I'll insert it in the measure values card there. I've added the new field right next to gross profit.\\n\\nBut it looks strange, doesn't it? If we divide gross profit by revenues, we would usually expect a number in the region of 10, 20, 30, or maybe 50%. Certainly not 1000. What is going on here? When I divide 71 million by 244 million, which is what we have in January, I obtain 29%, approximately.\\n\\nThere are two possible explanations. Either Tableau miscalculated the simple division we asked it to perform, or our formula is not 100% correct. It's most likely us and not the computer, right? I'll take out the GM percent field from the measure values box, and we'll edit the calculated field from here.\\n\\nWhat we forgot to do is type SUM around the two variables. If we don't sum the variables, we are not dividing their total figures for each month. Let's adjust our calculated field in this way and see what happens. OK?\\n\\nThis is a column with numbers that look like zeros, but perhaps these are percentage values. Let's change the way the GM percent column is displayed. To do that, I'll simply click on the GM percent variable in the measure values card and select the format option. We have quite a few options available here, so I'll simply select a percentage format with one decimal place. Voila, our table is ready.\\n\\nIn our next lesson, we'll add a filter that would allow us to choose whether to see both 2016 and 2017 values or just one\\n----------------------\\nDocument 2\\nSection Title: N/A\\nLecture Title: N/A\\nContent: Most executives, and by most, I mean all of them, prefer receiving tables that contain totals and subtotals. This makes it easier for them and helps them digest information in a faster way. In this lesson, we will learn how to add totals and subtotals to our Tableau tables.\\n\\nThat's nice because the table we have been working on in the last few videos doesn't contain totals for 2016 and 2017. We'll add them in this video. That's fairly easy to do. I'll go to Analysis, then Totals, and we'll select Show Column Grand Totals.\\n\\nThe totals we just added are for the entire period of 2016 and 2017. That's not really useful, is it? Whenever a person is interested in a company's sales or gross profit, they want to know how much it made in a specific year and whether it performed better than the year before. A company will also want to know how its year's sales compare to other companies.\\n\\nSo, I'll go to Analysis, Totals, and unclick the Show Column Grand Totals to remove the total we just added. What I can do differently the second time around is opt for Add All Subtotals. This will give me the total figures for 2016 and 2017 separately. Quite nice, right?\\n\\nNow, we can read the table easily. The fictitious numbers we see here help us compare 2016 and 2017 and give us a good idea of what the actual sales were for that period. In our next lesson, we'll add percentage gross margin to our table. Thanks for watching.\", additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":40}],"source":["result_improved"]},{"cell_type":"markdown","id":"3033f395-8cfc-404c-a2f7-e07c023f04d5","metadata":{"id":"3033f395-8cfc-404c-a2f7-e07c023f04d5"},"source":["### Stream the Response"]},{"cell_type":"markdown","source":["The improved retrieval chain *chain_retrieving_improved* is now streaming the response instead of returning it all at once. The for-loop prints each chunk as it is generated, allowing real-time streaming (useful for chat interfaces).\n","\n","Unlike previous steps, which focused on retrieving and formatting context, this step sends the structured input to the LLM and receives a generated answer based on the retrieved information. Hence, this is the final step of RAG process."],"metadata":{"id":"WUnjhFqeZRl_"},"id":"WUnjhFqeZRl_"},{"cell_type":"code","execution_count":41,"id":"8381358c-d8aa-4ccd-8aab-33576ac78ec5","metadata":{"id":"8381358c-d8aa-4ccd-8aab-33576ac78ec5","executionInfo":{"status":"ok","timestamp":1742238083530,"user_tz":-60,"elapsed":30,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}}},"outputs":[],"source":["result_streamed = chain_retrieving_improved.stream({\n","    \"question_lecture\": \"Adding a custom calculation\",\n","    \"question_title\": \"Why are we using SUM here? It's unclear to me.\",\n","    \"question_body\": \"This question refers to calculating the GM%.\"\n","})"]},{"cell_type":"code","execution_count":42,"id":"75e25ec6-4ca1-4d11-9bdb-a4e2453e4c95","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75e25ec6-4ca1-4d11-9bdb-a4e2453e4c95","executionInfo":{"status":"ok","timestamp":1742238085733,"user_tz":-60,"elapsed":572,"user":{"displayName":"Osman Alenbey","userId":"06601504222135124722"}},"outputId":"fa6212e7-fefb-4db5-dda2-63cfa33d9423"},"outputs":[{"output_type":"stream","name":"stdout","text":["messages=[SystemMessage(content='You will receive a question from a student taking a Tableau course, which includes a title and a body. \\nThe corresponding lecture will also be provided.\\n\\nAnswer the question using only the provided context.\\n\\nAt the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \\nResources: \\nSection: *Section Title*, Lecture: *Lecture Title* \\n...\\nReplace *Section Title* and *Lecture Title* with the appropriate titles.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"This is the question:\\nLecture: Adding a custom calculation\\nTitle: Why are we using SUM here? It's unclear to me.\\nBody: This question refers to calculating the GM%.\\n\\nThis is the context:\\nDocument 1\\nSection Title: N/A\\nLecture Title: N/A\\nContent: Ok, excellent. We're doing good. We've seen quite a few interesting Tableau tools so far, and we'll continue to do so during this lesson as well.\\n\\nOur table is almost ready. We have revenue, COGS, and gross profit. Now that I think about it, one thing we should probably add is a gross margin calculation right next to the gross profit figures. Gross margin is useful because it allows us to see what portion of revenues were converted into gross profit once we have considered the cost of goods sold.\\n\\nRight. Let's add a new calculated field. I'll name it GM percent. All we have to do is divide gross profits by revenue, right? And we already know how to do that. Ok, here we are. We've calculated a new field. Let's add it to the table. I'll insert it in the measure values card there. I've added the new field right next to gross profit.\\n\\nBut it looks strange, doesn't it? If we divide gross profit by revenues, we would usually expect a number in the region of 10, 20, 30, or maybe 50%. Certainly not 1000. What is going on here? When I divide 71 million by 244 million, which is what we have in January, I obtain 29%, approximately.\\n\\nThere are two possible explanations. Either Tableau miscalculated the simple division we asked it to perform, or our formula is not 100% correct. It's most likely us and not the computer, right? I'll take out the GM percent field from the measure values box, and we'll edit the calculated field from here.\\n\\nWhat we forgot to do is type SUM around the two variables. If we don't sum the variables, we are not dividing their total figures for each month. Let's adjust our calculated field in this way and see what happens. OK?\\n\\nThis is a column with numbers that look like zeros, but perhaps these are percentage values. Let's change the way the GM percent column is displayed. To do that, I'll simply click on the GM percent variable in the measure values card and select the format option. We have quite a few options available here, so I'll simply select a percentage format with one decimal place. Voila, our table is ready.\\n\\nIn our next lesson, we'll add a filter that would allow us to choose whether to see both 2016 and 2017 values or just one\\n----------------------\\nDocument 2\\nSection Title: N/A\\nLecture Title: N/A\\nContent: Most executives, and by most, I mean all of them, prefer receiving tables that contain totals and subtotals. This makes it easier for them and helps them digest information in a faster way. In this lesson, we will learn how to add totals and subtotals to our Tableau tables.\\n\\nThat's nice because the table we have been working on in the last few videos doesn't contain totals for 2016 and 2017. We'll add them in this video. That's fairly easy to do. I'll go to Analysis, then Totals, and we'll select Show Column Grand Totals.\\n\\nThe totals we just added are for the entire period of 2016 and 2017. That's not really useful, is it? Whenever a person is interested in a company's sales or gross profit, they want to know how much it made in a specific year and whether it performed better than the year before. A company will also want to know how its year's sales compare to other companies.\\n\\nSo, I'll go to Analysis, Totals, and unclick the Show Column Grand Totals to remove the total we just added. What I can do differently the second time around is opt for Add All Subtotals. This will give me the total figures for 2016 and 2017 separately. Quite nice, right?\\n\\nNow, we can read the table easily. The fictitious numbers we see here help us compare 2016 and 2017 and give us a good idea of what the actual sales were for that period. In our next lesson, we'll add percentage gross margin to our table. Thanks for watching.\", additional_kwargs={}, response_metadata={})]"]}],"source":["# Create a for-loop to stream the response\n","for chunk in result_streamed:\n","    print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":null,"id":"df39b05c-00e5-40da-ac9e-53de17334b42","metadata":{"id":"df39b05c-00e5-40da-ac9e-53de17334b42"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"cc848ace-bee5-45e0-a6f1-00c2e6ce7a00","metadata":{"id":"cc848ace-bee5-45e0-a6f1-00c2e6ce7a00"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"513a2594-aa62-4242-b403-452e848427d4","metadata":{"id":"513a2594-aa62-4242-b403-452e848427d4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"494540aa-6ccc-471c-86b7-44a602f36938","metadata":{"id":"494540aa-6ccc-471c-86b7-44a602f36938"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2e7b2d26-4b3c-4a0c-b4a1-52e3fb84739b","metadata":{"id":"2e7b2d26-4b3c-4a0c-b4a1-52e3fb84739b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"langchain_env_project","language":"python","name":"langchain_env_project"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}